{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SERMONE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK Part of Speech (POS)\n",
    "\n",
    "https://nlp.stanford.edu/software/tagger.shtml\n",
    "\n",
    "http://www.corpus.unam.mx/cursopln/plnPython/clase10.pdf\n",
    "\n",
    "https://stanfordnlp.github.io/stanfordnlp/\n",
    "\n",
    "https://stanfordnlp.github.io/CoreNLP/human-languages.html\n",
    "\n",
    "https://medium.com/analytics-vidhya/introduction-to-stanfordnlp-an-nlp-library-for-53-languages-with-python-code-d7c3efdca118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/dataset_procesado.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import languages neural pipeline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once\n",
    "import stanfordnlp\n",
    "\n",
    "# stanfordnlp.download('es')   # This downloads the Spanish models for the neural pipeline\n",
    "# stanfordnlp.download('en')   # This downloads the English models for the neural pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"missing_info\"] = df[\"missing_info\"].astype(str)\n",
    "df[\"missing_info_es\"] = df[\"missing_info_es\"].astype(str)\n",
    "df[\"missing_info_en\"] = df[\"missing_info_en\"].astype(str)\n",
    "\n",
    "df[\"improvements\"] = df[\"improvements\"].astype(str)\n",
    "df[\"improvements_es\"] = df[\"improvements_es\"].astype(str)\n",
    "df[\"improvements_en\"] = df[\"improvements_en\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the text columns (cleaned) for the corpus\n",
    "txt = \"\\n\".join(df[\"missing_info\"].dropna().values) + '\\n' + \"\\n\".join(df[\"improvements\"].dropna().values)\n",
    "txt_es = \"\\n\".join(df[\"missing_info_es\"].dropna().values) + '\\n' + \"\\n\".join(df[\"improvements_es\"].dropna().values)\n",
    "txt_en = \"\\n\".join(df[\"missing_info_en\"].dropna().values) + '\\n' + \"\\n\".join(df[\"improvements_en\"].dropna().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/julio/stanfordnlp_resources/es_ancora_models/es_ancora_tokenizer.pt', 'lang': 'es', 'shorthand': 'es_ancora', 'mode': 'predict'}\n",
      "---\n",
      "Loading: mwt\n",
      "With settings: \n",
      "{'model_path': '/home/julio/stanfordnlp_resources/es_ancora_models/es_ancora_mwt_expander.pt', 'lang': 'es', 'shorthand': 'es_ancora', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/julio/stanfordnlp_resources/es_ancora_models/es_ancora_lemmatizer.pt', 'lang': 'es', 'shorthand': 'es_ancora', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/julio/stanfordnlp_resources/es_ancora_models/es_ancora_tagger.pt', 'pretrain_path': '/home/julio/stanfordnlp_resources/es_ancora_models/es_ancora.pretrain.pt', 'batch_size': 1178675, 'lang': 'es', 'shorthand': 'es_ancora', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n",
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': '/home/julio/stanfordnlp_resources/en_ewt_models/en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': '/home/julio/stanfordnlp_resources/en_ewt_models/en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': '/home/julio/stanfordnlp_resources/en_ewt_models/en_ewt_tagger.pt', 'pretrain_path': '/home/julio/stanfordnlp_resources/en_ewt_models/en_ewt.pretrain.pt', 'batch_size': 1030319, 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# MODELS_DIR = \"/home/omar/Documentos/S_AI/Equipo_4/stanford_tagger/stanfordnlp_resources\"\n",
    "MODELS_DIR = \"/home/julio/stanfordnlp_resources\"\n",
    "\n",
    "# This sets up a default neural pipeline in Spanish\n",
    "nlp_es = stanfordnlp.Pipeline(lang='es', models_dir=MODELS_DIR, processors = \"tokenize,mwt,lemma,pos\", pos_batch_size=len(txt))\n",
    "nlp_en = stanfordnlp.Pipeline(lang='en', models_dir=MODELS_DIR, processors = \"tokenize,mwt,lemma,pos\", pos_batch_size=len(txt_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Bag of words with POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary to hold pos tags and their explanations\n",
    "pos_dict = {\n",
    "'CC': 'coordinating conjunction',\n",
    "'CD': 'cardinal digit',\n",
    "'DT': 'determiner',\n",
    "'EX': 'existential there (like: \\\"there is\\\" ... think of it like \\\"there exists\\\")',\n",
    "'FW': 'foreign word',\n",
    "'IN':  'preposition/subordinating conjunction',\n",
    "'JJ': 'adjective \\'big\\'',\n",
    "'JJR': 'adjective, comparative \\'bigger\\'',\n",
    "'JJS': 'adjective, superlative \\'biggest\\'',\n",
    "'LS': 'list marker 1)',\n",
    "'MD': 'modal could, will',\n",
    "'NN': 'noun, singular \\'desk\\'',\n",
    "'NNS': 'noun plural \\'desks\\'',\n",
    "'NNP': 'proper noun, singular \\'Harrison\\'',\n",
    "'NNPS': 'proper noun, plural \\'Americans\\'',\n",
    "'PDT': 'predeterminer \\'all the kids\\'',\n",
    "'POS': 'possessive ending parent\\'s',\n",
    "'PRP': 'personal pronoun I, he, she',\n",
    "'PRP$': 'possessive pronoun my, his, hers',\n",
    "'RB': 'adverb very, silently,',\n",
    "'RBR': 'adverb, comparative better',\n",
    "'RBS': 'adverb, superlative best',\n",
    "'RP': 'particle give up',\n",
    "'TO': 'to go \\'to\\' the store.',\n",
    "'UH': 'interjection errrrrrrrm',\n",
    "'VB': 'verb, base form take',\n",
    "'VBD': 'verb, past tense took',\n",
    "'VBG': 'verb, gerund/present participle taking',\n",
    "'VBN': 'verb, past participle taken',\n",
    "'VBP': 'verb, sing. present, non-3d take',\n",
    "'VBZ': 'verb, 3rd person sing. present takes',\n",
    "'WDT': 'wh-determiner which',\n",
    "'WP': 'wh-pronoun who, what',\n",
    "'WP$': 'possessive wh-pronoun whose',\n",
    "'WRB': 'wh-abverb where, when',\n",
    "'QF' : 'quantifier, bahut, thoda, kam (Hindi)',\n",
    "'VM' : 'main verb',\n",
    "'PSP' : 'postposition, common in indian langs',\n",
    "'DEM' : 'demonstrative, common in indian langs'\n",
    "}\n",
    "\n",
    "def extract_pos(doc):\n",
    "    parsed_text = {'word':[], 'pos':[], 'exp':[]}\n",
    "    for sent in doc.sentences:\n",
    "        for wrd in sent.words:\n",
    "            if wrd.pos in pos_dict.keys():\n",
    "                pos_exp = pos_dict[wrd.pos]\n",
    "            else:\n",
    "                pos_exp = 'NA'\n",
    "            parsed_text['word'].append(wrd.text)\n",
    "            parsed_text['pos'].append(wrd.pos)\n",
    "            parsed_text['exp'].append(pos_exp)\n",
    "    return pd.DataFrame(parsed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.\n"
     ]
    }
   ],
   "source": [
    "# Create corpus\n",
    "corpus = nlp_es(txt)\n",
    "corpus_es = nlp_es(txt_es)\n",
    "corpus_en = nlp_en(txt_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words dataframe\n",
    "BOW = extract_pos(corpus)\n",
    "BOW_ES = extract_pos(corpus_es)\n",
    "BOW_EN = extract_pos(corpus_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words of original text\n",
    "\n",
    "BOW.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words of spanish translated text\n",
    "\n",
    "BOW_ES.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of words of english translated text\n",
    "\n",
    "BOW_EN.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_en = BOW_EN[BOW_EN[\"exp\"].str.contains(\"adjective\")][\"pos\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW.drop([\"exp\"], axis=1, inplace=True)\n",
    "BOW_ES.drop([\"exp\"], axis=1, inplace=True)\n",
    "BOW_EN.drop([\"exp\"], axis=1, inplace=True)\n",
    "BOW.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for bow, lang in [(BOW, 'orig'), (BOW_ES, 'es'), (BOW_EN, 'en')]:\n",
    "    print(\"Language:\", lang)\n",
    "    for col in bow:\n",
    "        unique = bow[col].nunique()\n",
    "        print('Unique values of {}: '.format(col), unique)\n",
    "        \n",
    "# Wrong output!!! outdated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BOW['pos'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BOW_ES['pos'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(BOW_EN['pos'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the df\n",
    "import copy\n",
    "BOW_final = copy.deepcopy(BOW)\n",
    "BOW_ES_final = copy.deepcopy(BOW_ES)\n",
    "BOW_EN_final = copy.deepcopy(BOW_EN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = [BOW, BOW_ES, BOW_EN][0]\n",
    "bow.loc[bow['pos'].isin(list(adj_en) + [\"ADJ\", \"JJ\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjects are highly informative of positive and negative sentiments\n",
    "\n",
    "BOW = BOW.loc[BOW['pos'].isin(list(adj_en) + [\"ADJ\"])]\n",
    "BOW_ES = BOW_ES.loc[BOW_ES['pos'].isin(list(adj_en) + [\"ADJ\"])]\n",
    "BOW_EN = BOW_EN.loc[BOW_EN['pos'].isin(list(adj_en) + [\"ADJ\"])]\n",
    "    \n",
    "# BOW.drop(adj_filter, axis=0 , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates and sorting\n",
    "\n",
    "BOW.drop_duplicates(subset =\"word\", inplace = True)\n",
    "BOW_ES.drop_duplicates(subset =\"word\", inplace = True)\n",
    "BOW_EN.drop_duplicates(subset =\"word\", inplace = True)\n",
    "\n",
    "\n",
    "BOW.sort_values(\"word\", inplace = True)\n",
    "BOW_ES.sort_values(\"word\", inplace = True)\n",
    "BOW_EN.sort_values(\"word\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW_EN.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for bow, lang in [(BOW, 'orig'), (BOW_ES, 'es'), (BOW_EN, 'en')]:\n",
    "    print(\"Language:\", lang)\n",
    "    for col in bow:\n",
    "        unique = bow[col].nunique()\n",
    "        print('Unique values of {}: '.format(col), unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv'ed for manual label\n",
    "BOW.to_csv('datasets/adjs_orig.csv', index = False)\n",
    "BOW_ES.to_csv('datasets/adjs_es.csv', index = False)\n",
    "BOW_EN.to_csv('datasets/adjs_en.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK Sentiment Analysis\n",
    "\n",
    "https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk\n",
    "\n",
    "https://towardsdatascience.com/basic-binary-sentiment-analysis-using-nltk-c94ba17ae386\n",
    "\n",
    "https://towardsdatascience.com/unsupervised-sentiment-analysis-a38bf1906483"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
